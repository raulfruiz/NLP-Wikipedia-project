{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Wikipedia.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1jROG_rSKfo_QF2OUymj9Vox6APnGUucf",
      "authorship_tag": "ABX9TyNd7H/e+OlfAe7F6lfv9xRA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raulfruiz/NLP-Wikipedia-project/blob/main/NLP_Wikipedia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using NLP to find similar Wikipedia profiles\n",
        "\n",
        "This script intends to show a step-by-step of how to use pandas, numpy and scikitlearn in order to get similar Wikipedia profiles from a particular famous person.\n",
        "\n",
        "The aim is to provide the name of a person in the list and the algorithim will return a set of people with a similar profile\n",
        "\n",
        "##Importing libraries and the dataset\n",
        "dataset can be found on:https://www.kaggle.com/sameersmahajan/people-wikipedia-data"
      ],
      "metadata": {
        "id": "aSwrCvL3wAGf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9q8t8XRhTG8"
      },
      "outputs": [],
      "source": [
        "# Getting access to Drive to get .csv file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Libraries needed, accesing and reading data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP-project/people_wiki.csv\")\n",
        "df.head(10) #Now data is visible and readeable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vectorizing the text \n",
        "\n",
        "Now the idea is to create a vector from words found on column 'text'\n",
        "\n",
        "To do so, the method **countVectorizer()** will be used. It takes each relevant word in this column and asign it an index.\n",
        "\n",
        "Once words have received their index, an array will be created with method\n",
        "**.toarray()**. Each element on the array will show the number of times each word is repeated during the text on the position assigned before.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yP56ifi_j6kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Create a CountVectorizer() object class\n",
        "# This way .fit() method can be used to assign an index to each word\n",
        "\n",
        "vect = CountVectorizer()\n",
        "vectorizer = vect.fit(df['text']) \n",
        "\n",
        "# making the text to fit in an array with indexes \n",
        "\n",
        "word_weight_vector = vectorizer.transform(df['text']) #takes fitted text\n",
        "# Now each index (word) has been transformed to a number\n",
        "#This number indicates the number of times that word (index) is repeated on the text passed as parameter"
      ],
      "metadata": {
        "id": "b6Fq94yFC7ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, there is a method that do both things (fit and transform) at the same time:"
      ],
      "metadata": {
        "id": "mHxobCgEZqHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "word_weight_array = vect.fit_transform(df['text'])\n",
        "# now we have a vector with the number of times each relevant word is repeated\n",
        "# there is a vector per each row on the data set\n",
        "# these vectors are generated from analyzing just the text in column 'text'\n",
        "# using this method makes execution way faster"
      ],
      "metadata": {
        "id": "78CHnWhxZx4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Nearest Neighbours algorithm\n",
        "\n",
        "Once we have created our vectors from the 'text' column, we apply "
      ],
      "metadata": {
        "id": "b7U7FKPMEPla"
      }
    }
  ]
}